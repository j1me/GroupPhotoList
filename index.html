<!DOCTYPE html>
<html>
<head>
    <title>Group Photo Face Detection</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <!-- Load TensorFlow.js first -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0"></script>
    <!-- Load face detection models -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.12/dist/face-api.js"></script>
    <!-- Load ONNX Runtime Web with specific version -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.min.js"></script>
    <!-- Load YOLOv8 dependencies -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
    <style>
        .drag-area { border: 2px dashed #4CAF50; transition: all 0.3s ease; }
        .drag-area.active { border-color: #2196F3; background-color: rgba(33, 150, 243, 0.1); }
        #temp-canvas { display: none; }
    </style>
</head>
<body class="bg-gray-100 min-h-screen">
    <div class="container mx-auto px-4 py-8">
        <h1 class="text-3xl font-bold text-center mb-8">Group Photo Face Detection</h1>
        <div class="max-w-2xl mx-auto">
            <!-- Add model selector -->
            <div class="mb-8">
                <label class="block text-gray-700 text-sm font-bold mb-2" for="model-select">
                    Select Face Detection Model
                </label>
                <select id="model-select" class="block w-full px-4 py-2 rounded border border-gray-300 focus:outline-none focus:border-blue-500">
                    <option value="faceapi">face-api.js (Default)</option>
                    <option value="blazeface">BlazeFace</option>
                    <option value="yolov8">YOLOv8 (Most Accurate)</option>
                </select>
            </div>
            <div class="drag-area bg-white p-8 rounded-lg shadow-md text-center mb-8">
                <div class="mb-4">
                    <svg class="mx-auto h-12 w-12 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" 
                              d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12"/>
                    </svg>
                </div>
                <p class="text-gray-600 mb-4">Drag & Drop your group photo here or</p>
                <input type="file" id="file-input" class="hidden" accept="image/*">
                <button onclick="document.getElementById('file-input').click()" 
                        class="bg-blue-500 hover:bg-blue-600 text-white font-semibold py-2 px-4 rounded transition duration-300">
                    Choose File
                </button>
            </div>
            <div id="loading" class="hidden text-center mb-8">
                <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-500 mx-auto"></div>
                <p class="mt-4 text-gray-600">Processing image...</p>
            </div>
            <div id="results" class="hidden">
                <h2 class="text-2xl font-semibold mb-4">Original Photo</h2>
                <img id="original-image" class="w-full rounded-lg shadow-md mb-8" src="" alt="Original photo">
                <h2 class="text-2xl font-semibold mb-4">Detected Faces</h2>
                <div id="faces-grid" class="grid grid-cols-2 md:grid-cols-3 gap-4"></div>
            </div>
        </div>
    </div>
    <canvas id="temp-canvas"></canvas>
    <script>
        let isModelLoaded = false;
        let faceApiModel = null;
        let blazeFaceModel = null;
        let yolov8Model = null;
        let currentModel = 'faceapi';

        // Initialize ONNX Runtime with WASM backend
        async function initOnnxRuntime() {
            try {
                console.log('Initializing ONNX Runtime...');
                
                // Configure WASM paths directly
                ort.env.wasm.wasmPaths = {
                    'ort-wasm.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort-wasm.wasm',
                    'ort-wasm-simd.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort-wasm-simd.wasm',
                    'ort-wasm-threaded.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort-wasm-threaded.wasm'
                };
                
                // Set additional WASM configurations
                ort.env.wasm.numThreads = navigator.hardwareConcurrency || 4;
                ort.env.wasm.simd = true;
                
                console.log('ONNX Runtime initialized with WASM config:', ort.env.wasm);
                return true;
            } catch (error) {
                console.error('Failed to initialize ONNX Runtime:', error);
                throw error;
            }
        }
        
        // Load YOLOv8 model
        async function loadYoloModel() {
            try {
                console.log('Loading YOLOv8 model...');
                const modelUrl = './models/face-detection-retail-0004.onnx';
                
                // Create session with specific options
                const sessionOptions = {
                    executionProviders: ['wasm'],
                    graphOptimizationLevel: 'all'
                };
                
                // Create inference session
                console.log('Creating inference session...');
                const session = await ort.InferenceSession.create(modelUrl, sessionOptions);
                console.log('YOLOv8 model loaded successfully');
                return session;
            } catch (error) {
                console.error('Failed to load YOLOv8 model:', error);
                throw error;
            }
        }
        
        // Update loadModel function
        async function loadModel() {
            try {
                console.log('Loading face detection models...');
                
                // Initialize TensorFlow.js
                await tf.ready();
                await tf.setBackend('webgl');
                console.log('TensorFlow.js initialized:', tf.getBackend());
                
                // Initialize ONNX Runtime and load YOLOv8
                try {
                    await initOnnxRuntime();
                    yolov8Model = await loadYoloModel();
                } catch (error) {
                    console.error('Error initializing ONNX Runtime:', error);
                    // Don't throw, allow other models to work
                }

                // Load face-api.js models with specific configuration
                const faceDetectionNet = faceapi.nets.ssdMobilenetv1;
                await faceDetectionNet.load('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/');
                
                // Configure face detection parameters
                const minConfidence = 0.5;
                const options = new faceapi.SsdMobilenetv1Options({ minConfidence });
                faceapi.detectAllFaces = (input) => faceDetectionNet.detectFaces(input, options);
                console.log('face-api.js models loaded successfully');

                // Load BlazeFace model with specific configuration
                blazeFaceModel = await blazeface.load({
                    maxFaces: 50,
                    scoreThreshold: 0.3,
                    iouThreshold: 0.3
                });
                console.log('BlazeFace model loaded successfully');

                isModelLoaded = true;
                console.log('All face detection models loaded successfully');
            } catch (error) {
                console.error('Error loading models:', error);
                throw error;
            }
        }

        // YOLOv8 preprocessing function
        async function preprocessImageForYolo(img) {
            return tf.tidy(() => {
                // Convert to tensor
                const tensor = tf.browser.fromPixels(img);
                
                // Resize maintaining aspect ratio
                const [height, width] = tensor.shape.slice(0, 2);
                const maxSize = 640;
                const scale = maxSize / Math.max(height, width);
                const newHeight = Math.round(height * scale);
                const newWidth = Math.round(width * scale);
                
                const resized = tf.image.resizeBilinear(tensor, [newHeight, newWidth]);
                
                // Pad to square
                const padHeight = maxSize - newHeight;
                const padWidth = maxSize - newWidth;
                const padded = tf.pad(resized, [
                    [0, padHeight],
                    [0, padWidth],
                    [0, 0]
                ]);
                
                // Normalize to [0, 1]
                const normalized = tf.div(padded, 255.0);
                
                // Transpose to [N, C, H, W] format
                return tf.transpose(normalized.expandDims(0), [0, 3, 1, 2]);
            });
        }

        // YOLOv8 detection function
        async function detectWithYolov8(img) {
            if (!yolov8Model) {
                throw new Error('YOLOv8 model not loaded. Please try another model.');
            }

            let tensor = null;
            try {
                tensor = await preprocessImageForYolo(img);
                const inputTensor = new ort.Tensor(
                    'float32',
                    tensor.dataSync(),
                    [1, 3, 640, 640]
                );

                // Run inference
                const feeds = { images: inputTensor };
                console.log('Running YOLOv8 inference...');
                const results = await yolov8Model.run(feeds);
                
                if (!results || !results.output0) {
                    throw new Error('Invalid output from YOLOv8 model');
                }
                
                const output = results.output0;  // Shape: [num_boxes, 6] (x1, y1, x2, y2, score, class)
                console.log('YOLOv8 inference completed, processing results...');
                
                // Post-process detections
                const detections = [];
                const numBoxes = output.dims[0];
                const data = output.data;
                
                for (let i = 0; i < numBoxes; i++) {
                    const base = i * 6;
                    const confidence = data[base + 4];
                    
                    if (confidence > 0.25) {  // Confidence threshold
                        // Get coordinates (normalized)
                        let [x1, y1, x2, y2] = data.slice(base, base + 4);
                        
                        // Denormalize coordinates to original image size
                        const scale = Math.min(640 / img.width, 640 / img.height);
                        const padX = (640 - img.width * scale) / 2;
                        const padY = (640 - img.height * scale) / 2;
                        
                        x1 = (x1 - padX) / scale;
                        y1 = (y1 - padY) / scale;
                        x2 = (x2 - padX) / scale;
                        y2 = (y2 - padY) / scale;
                        
                        // Ensure coordinates are within image bounds
                        x1 = Math.max(0, Math.min(img.width, x1));
                        y1 = Math.max(0, Math.min(img.height, y1));
                        x2 = Math.max(0, Math.min(img.width, x2));
                        y2 = Math.max(0, Math.min(img.height, y2));
                        
                        detections.push({
                            detection: {
                                box: {
                                    x: x1,
                                    y: y1,
                                    width: x2 - x1,
                                    height: y2 - y1
                                },
                                score: confidence
                            }
                        });
                    }
                }
                
                console.log(`YOLOv8 found ${detections.length} faces`);
                return detections;
            } catch (error) {
                console.error('Error in YOLOv8 detection:', error);
                throw new Error(`YOLOv8 detection failed: ${error.message}`);
            } finally {
                // Clean up tensors
                if (tensor) {
                    tensor.dispose();
                }
            }
        }

        // Handle model selection change
        document.getElementById('model-select').addEventListener('change', (e) => {
            currentModel = e.target.value;
            console.log('Switched to model:', currentModel);
        });

        // Process detections and create face crops
        function processDetections(detections, image, modelType) {
            facesGrid.innerHTML = '';
            console.log('Processing detections:', detections);
            
            detections.forEach((detection, index) => {
                const box = detection.detection.box;
                
                // Add padding around face
                const padding = Math.min(box.width, box.height) * 0.2;
                const paddedX = Math.max(0, box.x - padding);
                const paddedY = Math.max(0, box.y - padding);
                const paddedWidth = Math.min(box.width + 2 * padding, image.width - paddedX);
                const paddedHeight = Math.min(box.height + 2 * padding, image.height - paddedY);

                // Create face canvas
                const faceCanvas = document.createElement('canvas');
                faceCanvas.width = paddedWidth;
                faceCanvas.height = paddedHeight;
                const faceCtx = faceCanvas.getContext('2d');
                faceCtx.drawImage(image, paddedX, paddedY, paddedWidth, paddedHeight, 0, 0, paddedWidth, paddedHeight);

                // Create face element
                const faceElement = document.createElement('div');
                faceElement.className = 'bg-white p-4 rounded-lg shadow-md';
                
                // Create face image with Google Lens search link
                const faceLink = document.createElement('a');
                faceLink.href = `https://lens.google.com/uploadbyurl?url=${encodeURIComponent(faceCanvas.toDataURL())}`;
                faceLink.target = '_blank';
                faceLink.innerHTML = `
                    <img src="${faceCanvas.toDataURL()}" class="w-full h-48 object-cover rounded" alt="Face ${index + 1}">
                    <p class="mt-2 text-center text-gray-600">Person ${index + 1}</p>
                `;
                
                faceElement.appendChild(faceLink);
                facesGrid.appendChild(faceElement);
            });

            loading.classList.add('hidden');
            results.classList.remove('hidden');
        }

        async function handleFiles(files) {
            if (!isModelLoaded) {
                alert('Please wait for the face detection models to load');
                return;
            }

            if (files.length === 0) return;
            
            const file = files[0];
            if (!file.type.startsWith('image/')) {
                alert('Please upload an image file');
                return;
            }

            loading.classList.remove('hidden');
            results.classList.add('hidden');

            try {
                console.log('Loading image...');
                const img = await loadImage(file);
                console.log('Image loaded, dimensions:', img.width, 'x', img.height);

                // Display original image
                originalImage.src = img.src;

                // Set canvas dimensions
                tempCanvas.width = img.width;
                tempCanvas.height = img.height;
                tempCtx.drawImage(img, 0, 0);

                console.log('Running face detection with model:', currentModel);
                try {
                    if (currentModel === 'faceapi') {
                        // face-api.js detection
                        const faceDetections = await faceapi.detectAllFaces(img);
                        console.log('Raw face-api.js detections:', faceDetections);
                        
                        if (faceDetections && faceDetections.length > 0) {
                            // Convert face-api.js format - box is directly on the detection object
                            const convertedDetections = faceDetections.map(det => ({
                                detection: {
                                    box: det._box // Access the internal box object
                                }
                            }));
                            processDetections(convertedDetections, img, 'faceapi');
                        } else {
                            throw new Error('No faces detected in the image');
                        }
                    } else if (currentModel === 'blazeface') {
                        // BlazeFace detection
                        const tensor = tf.tidy(() => {
                            // Convert image to tensor (returns a tensor of shape [height, width, 3])
                            return tf.browser.fromPixels(img);
                        });
                        
                        try {
                            // BlazeFace handles preprocessing internally
                            const predictions = await blazeFaceModel.estimateFaces(tensor, false);
                            console.log('BlazeFace raw predictions:', predictions);
                            
                            if (predictions && predictions.length > 0) {
                                const convertedDetections = predictions.map(pred => ({
                                    detection: {
                                        box: {
                                            x: pred.topLeft[0],
                                            y: pred.topLeft[1],
                                            width: pred.bottomRight[0] - pred.topLeft[0],
                                            height: pred.bottomRight[1] - pred.topLeft[1]
                                        }
                                    }
                                }));
                                processDetections(convertedDetections, img, 'blazeface');
                            } else {
                                throw new Error('No faces detected in the image');
                            }
                        } finally {
                            tensor.dispose();
                        }
                    } else if (currentModel === 'yolov8') {
                        // YOLOv8 detection
                        if (!yolov8Model) {
                            throw new Error('YOLOv8 model not loaded. Please try another model.');
                        }
                        
                        const detections = await detectWithYolov8(img);
                        console.log('YOLOv8 detections:', detections);
                        
                        if (detections && detections.length > 0) {
                            processDetections(detections, img, 'yolov8');
                        } else {
                            throw new Error('No faces detected in the image');
                        }
                    }
                } catch (detectionError) {
                    console.error('Face detection error:', detectionError);
                    throw new Error(`Face detection failed: ${detectionError.message}`);
                }
            } catch (error) {
                console.error('Error processing image:', error);
                loading.classList.add('hidden');
                alert('An error occurred while processing the image: ' + error.message);
            }
        }

        // Wait for page to load before initializing
        window.addEventListener('load', async () => {
            console.log('Page loaded, starting model initialization...');
            try {
                await loadModel();
                console.log('Models loaded successfully');
            } catch (error) {
                console.error('Failed to initialize models:', error);
                alert('Failed to load face detection models. Please try refreshing the page.');
            }
        });

        const dragArea = document.querySelector('.drag-area');
        const fileInput = document.getElementById('file-input');
        const loading = document.getElementById('loading');
        const results = document.getElementById('results');
        const originalImage = document.getElementById('original-image');
        const facesGrid = document.getElementById('faces-grid');
        const tempCanvas = document.getElementById('temp-canvas');
        const tempCtx = tempCanvas.getContext('2d');

        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {
            dragArea.addEventListener(eventName, preventDefaults, false);
        });

        function preventDefaults(e) {
            e.preventDefault();
            e.stopPropagation();
        }

        ['dragenter', 'dragover'].forEach(eventName => {
            dragArea.addEventListener(eventName, () => dragArea.classList.add('active'));
        });

        ['dragleave', 'drop'].forEach(eventName => {
            dragArea.addEventListener(eventName, () => dragArea.classList.remove('active'));
        });

        dragArea.addEventListener('drop', handleDrop, false);
        fileInput.addEventListener('change', handleFileSelect, false);

        function handleDrop(e) {
            handleFiles(e.dataTransfer.files);
        }

        function handleFileSelect(e) {
            handleFiles(e.target.files);
        }

        async function loadImage(file) {
            return new Promise((resolve, reject) => {
                const img = new Image();
                img.onload = () => resolve(img);
                img.onerror = reject;
                img.src = URL.createObjectURL(file);
            });
        }
    </script>
</body>
</html>
